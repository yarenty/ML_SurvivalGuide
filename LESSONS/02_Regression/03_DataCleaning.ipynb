{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b91a74ba-85f4-486e-b5f9-d0898f0626bf",
    "_uuid": "6ac53f18b4f4ec0fc44348cedb5d1c319fa127c0"
   },
   "source": [
    "# Another data cleaning techinique\n",
    "\n",
    "Data scaling and normalization (and what the difference is between the two).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5cd5061f-ae30-4837-a53b-690ffd5c5830",
    "_uuid": "9d82bf13584b8e682962fbb96131f2447d741679"
   },
   "source": [
    "\n",
    "\n",
    "The first thing we'll need to do is load in the libraries and datasets we'll be using.\n",
    "\n",
    "**Note** If you have error about missing libraries - just remember to stop jupyter, and install it by our previous lesson `pip install your-missing-library`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "135a7804-b5f5-40aa-8657-4a15774e3666",
    "_uuid": "835cbe0834b935fb0fd40c75b9c39454836f4d5f"
   },
   "outputs": [],
   "source": [
    "# modules we'll use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for Box-Cox Transformation\n",
    "from scipy import stats\n",
    "\n",
    "# for min_max scaling\n",
    "from mlxtend.preprocessing import minmax_scaling\n",
    "\n",
    "# plotting modules\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read in all our data\n",
    "houses = pd.read_csv(\"../DATA/houses.csv\")\n",
    "\n",
    "# set seed for reproducibility\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "62b9f021-5b80-43e2-bf60-8e0d5e22d572",
    "_uuid": "032a618abb98a28e60ab84376cf21402178f995d"
   },
   "source": [
    "# Scaling vs. Normalization: What's the difference?\n",
    "____\n",
    "\n",
    "One of the reasons that it's easy to get confused between scaling and normalization is because the terms are sometimes used interchangeably and they are very similar!  \n",
    "In both cases, you're transforming the values of numeric variables so that the transformed data points have specific helpful properties. The difference is that, in scaling, you're changing the *range* of your data while in normalization you're changing the *shape of the distribution* of your data. \n",
    "\n",
    "\n",
    "___\n",
    "\n",
    "## **Scaling**\n",
    "\n",
    "This means that you're transforming your data so that it fits within a specific scale, like 0-100 or 0-1.  You want to scale data when you're using methods based on measures of how far apart data points, like [support vector machines, or SVM](https://en.wikipedia.org/wiki/Support_vector_machine) or [k-nearest neighbors, or KNN](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm). With these algorithms, a change of \"1\" in any numeric feature is given the same importance. \n",
    "\n",
    "For example, you might be looking at the prices of some products in both Yen and US Dollars. One US Dollar is worth about 100 Yen, but if you don't scale your prices methods like SVM or KNN will consider a difference in price of 1 Yen as important as a difference of 1 US Dollar!  With currency, you can convert between currencies. But what about if you're looking at something like height and weight?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0942c00-e306-4c64-a53a-e76d07cd937f",
    "_uuid": "e35280c753de7b963c4d812624c816c766ef4367"
   },
   "outputs": [],
   "source": [
    "# generate 1000 data points randomly drawn from an exponential distribution\n",
    "original_data = np.random.exponential(size = 1000)\n",
    "\n",
    "# mix-max scale the data between 0 and 1\n",
    "scaled_data = minmax_scaling(original_data, columns = [0])\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ed530656-2707-4978-835c-c665a9e25ec0",
    "_uuid": "a2523383e47af8d7902b75c5da7829b85553dcae"
   },
   "source": [
    "The *shape* of the data doesn't change, but instead of ranging from 0 to 8ish, it now ranges from 0 to 1.\n",
    "\n",
    "___\n",
    "## Normalization\n",
    "\n",
    "Scaling just changes the range of your data. Normalization is a more radical transformation. The point of normalization is to change your observations so that they can be described as a normal distribution.\n",
    "\n",
    "> **[Normal distribution:](https://en.wikipedia.org/wiki/Normal_distribution)** - the \"bell curve\", this is a specific statistical distribution where a roughly equal observations fall above and below the mean, the mean and the median are the same, and there are more observations closer to the mean. The normal distribution = the Gaussian distribution.\n",
    "\n",
    "In general, you'll only want to normalize your data if you're going to be using a machine learning or statistics technique that assumes your data is normally distributed, like:t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA), Gaussian naive Bayes...\n",
    "\n",
    "The method were  using to normalize here is called the [Box-Cox Transformation](https://en.wikipedia.org/wiki/Power_transform#Box%E2%80%93Cox_transformation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "851dc531-ea15-46f4-ba59-2e9be614856c",
    "_uuid": "e1484f70203b1a9335a557939398beb45b3a4fbd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize the exponential data with boxcox\n",
    "normalized_data = stats.boxcox(original_data)\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(original_data, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_data[0], ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "52011105-e1e3-4bb0-9b59-59614a96e3d4",
    "_uuid": "5975eb63a310ca983facc4a8b969e235fee58c74"
   },
   "source": [
    "Notice that the *shape* of our data has changed. Before normalizing it was L-shaped, after normalizing it looks  like \"bell curve\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc728697-ce3e-4890-b14d-597b2281f30d",
    "_uuid": "0c4d06413046e632dd1936095028587af3be0e47"
   },
   "source": [
    "# Scaling\n",
    "___\n",
    "\n",
    "Let's start by scaling the goals of each campaign, which is how much money they were asking for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "aa2f45a8-b7ce-455e-8f8b-9ba24a1cdfeb",
    "_uuid": "9c6aaa573dbd346106b120c499b967718919d520"
   },
   "outputs": [],
   "source": [
    "# select the LotArea column\n",
    "lotArea = houses.LotArea.dropna()\n",
    "\n",
    "# scale the goals from 0 to 1\n",
    "scaled_data = minmax_scaling(lotArea, columns = [0])\n",
    "\n",
    "# plot the original & scaled data together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(houses.LotArea.dropna(), ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(scaled_data, ax=ax[1])\n",
    "ax[1].set_title(\"Scaled data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1e0a0591-ed82-4a9f-bca3-356706921606",
    "_uuid": "71d69ec4508b1b7048cd9592605e17884e6aed25"
   },
   "source": [
    "Scaling changed the scales of the plots dramatically (but not the shape of the data: it looks like most hauses have typical aresa but a few have very large ones)\n",
    "\n",
    "**Challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bdaf7f43-cbbe-45fa-844f-fbe268cc90d7",
    "_uuid": "6ab743a9bb0a40ca7921fc506f39f41217e47ab3"
   },
   "outputs": [],
   "source": [
    "# Challenge\n",
    "# We just scaled the \"LotArea\" column. What about the \"LotFrontage\" column?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "774002fa-e98f-4ab8-a00e-943636df5214",
    "_uuid": "e19939624c42f1e3a0ca371d883ac417adb31ab7"
   },
   "source": [
    "# Normalization\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "83e5dd3e-9d5e-4e75-8cc2-da38b6a76a8f",
    "_uuid": "4b45fd281c4b2004ad9e02b7b4391100cca7023a"
   },
   "outputs": [],
   "source": [
    "# get only positive pledges (using their indexes)\n",
    "positive_pledges = houses.LotArea \n",
    "\n",
    "# normalize the pledges (w/ Box-Cox)\n",
    "normalized_pledges = stats.boxcox(positive_pledges)[0]\n",
    "\n",
    "# plot both together to compare\n",
    "fig, ax=plt.subplots(1,2)\n",
    "sns.distplot(positive_pledges, ax=ax[0])\n",
    "ax[0].set_title(\"Original Data\")\n",
    "sns.distplot(normalized_pledges, ax=ax[1])\n",
    "ax[1].set_title(\"Normalized data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d090a28b-bf0c-4da5-be6e-ce57b3e756be",
    "_uuid": "06252c91946d610e8487023d1c8fff79a8a4677f"
   },
   "source": [
    "It's not perfect but it is much closer to normal\n",
    "\n",
    "\n",
    "**Challenge**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c00a1bd8-1743-4954-8ea5-139a2bcabd60",
    "_uuid": "6dd21ff124b05826e5ef104f44e1dbf055154e2f"
   },
   "outputs": [],
   "source": [
    "# Challenge \n",
    "# We looked as the LotArea column. What about the \"LotFrontage\" column? Does it have the same info?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = houses.LotArea\n",
    "y = houses.SalePrice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split( normalized_pledges, y, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(X_train[:, np.newaxis], Y_train)\n",
    "\n",
    "Y_pred = model.predict(X_test[:, np.newaxis])\n",
    "\n",
    "plt.scatter(X_train, Y_train)\n",
    "plt.plot(X_test, Y_pred, color='#FF0000');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test[:, np.newaxis],Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HA!! If you remember previous lessons - you can see that using normalization we just double score on single input!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADVANCED CHALLENGE**   \n",
    "Can you apply the same for 4 columns used in previous lesson??\n",
    "\n",
    "possible tip: you may use `np.asarray([c1],[c2],[c3])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = houses[['LotFrontage','LotArea','OverallQual','OverallCond','SalePrice']].dropna()\n",
    "\n",
    "# your code: normalize all 4 input columns\n",
    "x2 = None\n",
    "\n",
    "y2 = H.SalePrice\n",
    "\n",
    "print(\"x2 shape:\", x2.shape)\n",
    "print(\"y2 shape:\",y2.shape)\n",
    "\n",
    "X2_train, X2_test, Y2_train, Y2_test = train_test_split( x2, y2, test_size=0.20, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "x2 shape: (1201, 4)\n",
    "y2 shape: (1201,)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model2.fit(X2_train, Y2_train)\n",
    "\n",
    "y2fit = model2.predict(X2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(X2_train[:,0].shape)\n",
    "print(Y2_train.shape)\n",
    "\n",
    "plt.scatter(X2_train[:,0], Y2_train , color='#2F08EC', marker=\"s\")\n",
    "plt.scatter(X2_test[:,0], Y2_test , color='#FF082C')\n",
    "plt.scatter(X2_test[:,0], y2fit , color='#00FF00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X2_test,Y2_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.score(X2_train,Y2_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "\n",
    "```\n",
    "test: 0.685395007040821\n",
    "train: 0.6779247339356669\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ADVANCED CHALLENGE 2**\n",
    "\n",
    "Now try the same but normalize only first 2 columns: LotFrontage and  LotArea while OverallQual and OverallCond leave in orginal condition...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code: # your code: normalize 2 first input columns\n",
    "x3 = None\n",
    "y3 = H.SalePrice\n",
    "\n",
    "## your code: split 80/20 %\n",
    "X3_train, X3_test, Y3_train, Y3_test = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model3.fit(X3_train, Y3_train)\n",
    "\n",
    "y3fit = model3.predict(X3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(X3_test,Y3_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.score(X3_train,Y3_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "\n",
    "```\n",
    "test: 0.7151474283355291\n",
    "train: 0.6929990270245685\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again! You are great again!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
