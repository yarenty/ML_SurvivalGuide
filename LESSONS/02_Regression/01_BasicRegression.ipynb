{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Regression\n",
    "\n",
    "\n",
    "Generally Machine learning allows us do the tasks that are too diﬃcult to solve with ﬁxed programs written/designed by humans.\n",
    "\n",
    "For example, if we want a robot to be able to walk, then we could program the robot to learn to walk, or we could attempt to directly write a program that speciﬁes how to walk manually. Machine learning are usually described in terms of how the machine learning system should process an example. An example is a collection of features that have been measured from some object/event that we want the machine learning system to process. For example,the features of an image are usually the values of the pixels in the image.\n",
    "\n",
    "Linear Regression: is to predict a numerical value given some input. \n",
    "\n",
    "An example of a regression task is the prediction of the expected claim amount that an insured person will make (used to set insurance premiums), or the prediction of future prices of securities. These kinds of predictions are also used for algorithmic trading.\n",
    "\n",
    "\n",
    "Here we will get few simple functions that are used in ML *every time*\n",
    "\n",
    "\n",
    "## Simple start\n",
    "\n",
    "Our assumption here is that we are dealing with very simple linear data that could be described as:\n",
    "```\n",
    " y = W*x + b\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn; seaborn.set()  # set plot style\n",
    "\n",
    "#Our input data ;-)\n",
    "x = np.array([1.,2.,3,4,5,6,7,8,9], float)\n",
    "y = np.array([1,3,2,3,5,4,6,4,7], float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Calculate Mean and Variance\n",
    "\n",
    "The first step is to estimate the mean and the variance of data variables.\n",
    "\n",
    "```\n",
    "mean(x) = sum(x) / count(x)\n",
    "```\n",
    "Below is a function named mean() that implements this behavior for a list of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean value of a list of numbers\n",
    "def mean(values):\n",
    "    return sum(values) / float(len(values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variance is the sum squared difference for each value from the mean value.\n",
    "\n",
    "Variance for a list of numbers can be calculated as:\n",
    "```\n",
    "variance = sum( (x - mean(x))^2 )\n",
    "```\n",
    "\n",
    "\n",
    "**Variance** is the expectation of the squared deviation of a random variable from its mean. Informally, it measures how far a set of numbers are spread out from their average value. Variance has a central role in statistics, where some ideas that use it include descriptive statistics, statistical inference, hypothesis testing, goodness of fit, and Monte Carlo sampling. Variance is an important tool in the sciences, where statistical analysis of data is common. The variance is the square of the standard deviation, the second central moment of a distribution, and the covariance of the random variable with itself.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Example of samples from two populations with the same mean but different variances. The red population has mean 100 and variance 100 (SD=10) while the blue population has mean 100 and variance 2500 (SD=50).](../images/variance.png)\n",
    "Example of samples from two populations with the same mean but different variances. The red population has mean 100 and variance 100 (SD=10) while the blue population has mean 100 and variance 2500 (SD=50).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance(val):\n",
    "    # challenge! what is mean of x\n",
    "    mean_x = None \n",
    "    var = .0\n",
    "    for x in val:\n",
    "        #challenge! tip:remember ^2 in python is **2\n",
    "        var += None \n",
    "    return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "expected: 60.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Covariance\n",
    "\n",
    "The covariance of two groups of numbers describes how those numbers change together.\n",
    "\n",
    "Covariance is a generalization of correlation. Correlation describes the relationship between two groups of numbers, whereas covariance can describe the relationship between two or more groups of numbers.\n",
    "\n",
    "Additionally, covariance can be normalized to produce a correlation value.\n",
    "\n",
    "Nevertheless, we can calculate the covariance between two variables as follows:\n",
    "\n",
    "```\n",
    "covariance = sum((x(i) - mean(x)) * (y(i) - mean(y)))\n",
    "```\n",
    "\n",
    "Below is a function named covariance() that implements this statistic. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate covariance between x and y\n",
    "def covariance(x, y):\n",
    "    # challenge : 2 lines on means :-)\n",
    "    mean_x = None\n",
    "    mean_y = None\n",
    "    \n",
    "    covar = 0.0\n",
    "    for i in range(len(x)):\n",
    "        # challenge! i is your index so accessing x of index i is x[i] \n",
    "        covar += None\n",
    "    return covar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covar = covariance(x, y)\n",
    "print('Covariance: %.3f' % (covar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: 36.000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Estimate Coefficients\n",
    "We must estimate the values for two coefficients in simple linear regression.\n",
    "\n",
    "The first is W which can be estimated as:\n",
    "\n",
    "```\n",
    "W = sum((x(i) - mean(x)) * (y(i) - mean(y))) / sum( (x(i) - mean(x))^2 )\n",
    "```\n",
    "Looking above we can simplify this to:\n",
    "\n",
    "```\n",
    "W = covariance(x, y) / variance(x)\n",
    "```\n",
    "We already have functions to calculate covariance() and variance().\n",
    "\n",
    "Next, we need to estimate a value for b, also called the intercept as it controls the starting point of the line where it intersects the y-axis.\n",
    "\n",
    "```\n",
    "b = mean(y) - W * mean(x)\n",
    "```\n",
    "Again, we know how to estimate W and we have a function to estimate mean().\n",
    "\n",
    "We can put all of this together into a function named coefficients() that takes the dataset as an argument and returns the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate coefficients\n",
    "def coefficients(x,y):\n",
    "    x_mean, y_mean = mean(x), mean(y)\n",
    "    # challenge! 2 lines - see above\n",
    "    W = None\n",
    "    b = None\n",
    "    return [W, b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W, b = coefficients(x,y)\n",
    "print('Coefficients: W=%.3f, b=%.3f' % (W,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected: W=0.600, b=0.889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check if everything is working as expected!\n",
    "\n",
    "Lets build out prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x,W,b):\n",
    "    p = list()\n",
    "    for i in x:\n",
    "        #challenge! do you remember our initial assumption? \n",
    "        y = None\n",
    "        p.append(y)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = predict(x,W,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets display our output data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y , color='#2F08EC', marker=\"s\")\n",
    "plt.scatter(x, y_ , color='#FF082C')\n",
    "plt.plot(x, y_ , color='#FF0000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We just build linear regression model!**\n",
    "\n",
    "Lets make it more \"offcial\".\n",
    "\n",
    "We will also add in a function to manage the evaluation of the predictions called evaluate_algorithm() and another function to estimate the Root Mean Squared Error of the predictions called rmse_metric().\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Root-mean-square error (RMSE) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSE represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSE serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSE is a measure of accuracy, to compare forecasting errors of different models for a particular data and not between datasets, as it is scale-dependent.\n",
    "\n",
    "RMSE is the square root of the average of squared errors. The effect of each error on RMSE is proportional to the size of the squared error; thus larger errors have a disproportionately large effect on RMSE. Consequently, RMSE is sensitive to outliers.\n",
    "\n",
    "\n",
    "![](../images/rsme.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "# Calculate root mean squared error\n",
    "def rmse_metric(actual, predicted):\n",
    "    sum_error = 0.0\n",
    "    for i in range(len(actual)):\n",
    "        prediction_error = predicted[i] - actual[i]\n",
    "        sum_error += (prediction_error ** 2)\n",
    "    mean_error = sum_error / float(len(actual))\n",
    "    return sqrt(mean_error)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate regression algorithm on training dataset\n",
    "def evaluate_algorithm(x, y, algorithm):\n",
    "    predicted = algorithm(x, y)\n",
    "    rmse = rmse_metric(y, predicted)\n",
    "    return rmse,predicted\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple linear regression algorithm\n",
    "def simple_linear_regression(x,y):\n",
    "    predictions = list()\n",
    "    W, b = coefficients(x,y)\n",
    "    for row in x:\n",
    "        yhat = W * row + b\n",
    "        predictions.append(yhat)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse,predictions = evaluate_algorithm(x, y, simple_linear_regression)\n",
    "print('RMSE: %.3f' % (rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use scikit library\n",
    "\n",
    "We can do the same using scikit-learn!\n",
    "\n",
    "*One trick here - scikit needs x to be 2D input - so we expand our x using numpy.newaxis*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "model = LinearRegression(fit_intercept=True)\n",
    "\n",
    "model.fit(x[:, np.newaxis], y)\n",
    "\n",
    "xfit = x.copy()\n",
    "yfit = model.predict(x[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit, color='#FF0000');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Coefficients: W=%.3f, b=%.3f' % (model.coef_[0],model.intercept_))\n",
    "\n",
    "print(sqrt(mean_squared_error(y,yfit)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Tensorflow version\n",
    "\n",
    "\n",
    "Lets see two versions here:\n",
    "- using LinearRegressor estimator \n",
    "- using GradientDescentOptimizer - how this works will be in next lesson\n",
    "\n",
    "\n",
    "In first example we will se first hyperparameter - epoch. An epoch is a full iteration over samples/train set. The number of epochs is how many times the algorithm is going to run. The number of epochs affects directly (or not) the result of the training step (with just a few epochs you can reach only a local minimum, but with more epochs, you can reach a global minimum or at least a better local minimum).\n",
    "If you set epoch to too small value - you will not get good results - underfitting, if you set to too big - depends on algorithm, you can get overfitting, but generally bigger is better;-).\n",
    "\n",
    "\n",
    "*Note: Tensorflow in general feels more complicated as is quite low level, but allows to build more advanced architectures. In future we will see how Keras is used on top of tensorflow to simplify development phase!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "column =  tf.feature_column.numeric_column('x')\n",
    "lin_reg = tf.estimator.LinearRegressor(feature_columns=[column])\n",
    "\n",
    "# Train the estimator\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(x={\"x\": x}, y=y, shuffle=False, num_epochs=100, batch_size=1)\n",
    "lin_reg.train(train_input)\n",
    "\n",
    "# Make predictions\n",
    "predict_input = tf.estimator.inputs.numpy_input_fn(x={\"x\": x}, num_epochs=1, shuffle=False)\n",
    "results = lin_reg.predict(predict_input)\n",
    "\n",
    "\n",
    "tf_pred=list()\n",
    " # Print result\n",
    "for value in results:\n",
    "    print(value['predictions'])\n",
    "    tf_pred.append(value['predictions'])\n",
    "        \n",
    "print(sqrt(mean_squared_error(y,tf_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge! \n",
    "# can you spot difference? if so change epochs to 400 \n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit, color='#FF0000');\n",
    "plt.plot(x, tf_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient descent** is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or of the approximate gradient) of the function at the current point. If instead one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.\n",
    "\n",
    "![](../images/gradient1.png)\n",
    "\n",
    "**Note** Gradient descent work on cost function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = tf.placeholder(tf.float32, [None,1])\n",
    "ty_ = tf.placeholder(tf.float32, [None,1])\n",
    "\n",
    "tw = tf.Variable(tf.zeros([1]))\n",
    "tb = tf.Variable(tf.zeros([1]))\n",
    "\n",
    "ty = tw*tx + tb\n",
    "\n",
    "cost  = tf.reduce_mean(tf.square(ty_- ty))\n",
    "\n",
    "#learning_rate 0.01\n",
    "train_step = tf.train.GradientDescentOptimizer(0.0001).minimize(cost)\n",
    "\n",
    "# initialize global variables and start session\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "for epoch in range(100):\n",
    "    for i in range(len(x)):\n",
    "        xs = np.array([[x[i]]])\n",
    "        ys = np.array([[y[i]]])\n",
    "        sess.run(train_step,feed_dict={ tx: xs, ty_:ys })\n",
    "    \n",
    "print(\"after %d iteration\" % epoch)\n",
    "print(\"w: %f\" % sess.run(tw))\n",
    "print(\"b: %f\" % sess.run(tb))\n",
    "\n",
    "tgy = x*sess.run(tw)+sess.run(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# challenge\n",
    "# can you spot difference? if so change learning_rate to 0.001 and epochs to 1000\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit, color='#FF0000');\n",
    "plt.plot(x, tgy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have another hyperparameter: **learning rate**.\n",
    "\n",
    "It's one of most important parameters - in simple words learning rate determines how fast coefficents (in case of linear regression or logistic regression) or weights (in case of a neural network) change.\n",
    "\n",
    "If c is a cost function with variables (or weights) w1,w2….wn then,\n",
    "Lets take stochastic gradient descent where we change weights sample by sample -\n",
    "\n",
    "```\n",
    "for every sample(\n",
    "   w1new = w1 + (learning rate)* (derivative of cost function w1)\n",
    ")\n",
    "```\n",
    "\n",
    "If learning rate is too high derivative may miss the 0 slope point or learning rate is too low then it may take forever to reach that point.\n",
    "\n",
    "![](../images/gradient2.png)\n",
    "\n",
    "\n",
    "So we need to figure out that balanced learning rate.  \n",
    "\n",
    "*Note: On top of gradient descent there are several more advanced optimizers (RMSProp, Adam, Momentum, Adadelta ...) which has additional hyperparameters.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2O version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "h2o.init(max_mem_size = \"2G\")             #specify max number of bytes. uses all cores by default.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "import pandas as pd\n",
    "#create input dataframe\n",
    "train = h2o.H2OFrame(pd.DataFrame(data={'x':x,'y':y}))\n",
    "\n",
    "glm = H2OGeneralizedLinearEstimator()\n",
    "glm.train(['x'],'y',training_frame=train) #inputs : list of X column, y column, and train frame\n",
    "glm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2oY = glm.predict(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y)\n",
    "plt.plot(x, yfit, color='#FF0000');\n",
    "plt.plot(x, h2oY.as_data_frame().predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional challenge:  what parameters can you change: solver, alpha, lambda ...?? \n",
    "\n",
    "help(H2OGeneralizedLinearEstimator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
